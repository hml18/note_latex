\documentclass[]{article}
\usepackage[backend=bibtex]{biblatex}
\usepackage[UTF8]{ctex}
\usepackage{caption}
\usepackage{graphicx,subfig}
\usepackage{listings}       % 插入代码需要
% 用来设置附录中代码的样式
\lstset{
	basicstyle          =   \sffamily,          % 基本代码风格
	keywordstyle        =   \bfseries,          % 关键字风格
	commentstyle        =   \rmfamily\itshape,  % 注释的风格，斜体
	stringstyle         =   \ttfamily,  % 字符串风格
	flexiblecolumns,                % 别问为什么，加上这个
	numbers             =   left,   % 行号的位置在左边
	showspaces          =   false,  % 是否显示空格，显示了有点乱，所以不现实了
	numberstyle         =   \zihao{-5}\ttfamily,    % 行号的样式，小五号，tt等宽字体
	showstringspaces    =   false,
	captionpos          =   t,      % 这段代码的名字所呈现的位置，t指的是top上面
	frame               =   lrtb,   % 显示边框
}

\lstdefinestyle{Python}{
	language        =   Python, % 语言选Python
	basicstyle      =   \zihao{-5}\ttfamily,
	numberstyle     =   \zihao{-5}\ttfamily,
	keywordstyle    =   \color{blue},
	keywordstyle    =   [2] \color{teal},
	stringstyle     =   \color{magenta},
	commentstyle    =   \color{red}\ttfamily,
	breaklines      =   true,   % 自动换行，建议不要写太长的行
	columns         =   fixed,  % 如果不加这一句，字间距就不固定，很丑，必须加
	basewidth       =   0.5em,
}


\begin{document}
\section{Modify MLM Task}
hard label is human annotated.a labor-intensive and time-consuming task. When training, we use cross entropy loss to compare the hard label and the predicted soft label. Is there a gap? I have tried to use \verb|torch.nn.CrossEntropyLoss()| to calc the ground truth and the predicted logit. 
% As depict in \ref{CrossEntropyLoss_test}, 按照我的想法，它的loss应该是0才对。
% \begin{figure}
%     \centering
%     \includegraphics[width=.8\textwidth]{}
%     \caption{test CrossEntropy function}
%     \img{CrossEntropyLoss_test}
% \end{figure}

the problem is how to get the appropriate soft label?

\section{}

\end{document}